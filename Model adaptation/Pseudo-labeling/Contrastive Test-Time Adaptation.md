本文提出的 AdaContrast 的两个关键：
1、一种利用自监督对比学习的新颖方法 $\longrightarrow$ 促进目标特征学习
2、一种新颖的在线伪标签细化方案 $\longrightarrow$ 对伪标签去噪

#### 在线伪标签细化
本文在每个 batch 的基础上预测和细化伪标签，而不是每个 epoch 之后，以便模型的逐步更新能够反映在最新的伪标签中。借助表征目标特征空间的内存队列 $Q_w$，由最近邻软投票实现细化

![image.png](https://papernote-1394983352.cos.ap-nanjing.myqcloud.com/tta-note-img/20260118153501481.png)

如图中 (a) 所示，在每个 batch 当中，给定一张目标图像 $x_t$，将其弱增强版本 $t_w(x_t)$ 编码为特征向量 $w = f_t(t_w(x_t))$，在目标特征空间中找到其最近邻，平均最近邻的预测概率并得到其伪标签

##### 内存队列
为实现最近邻搜索，维护一个长度为 M 的存储弱增强目标样本的特征和预测概率的内存队列 $Q_w = \left \{w^{'j}, p^{'j}\right \}_{j=1}^M$，并用当前的 batch 进行即时更新。该队列使用 M 个随机挑选的目标样本初始化，以类似 Moco 的方式出队和入队。动量模型在适应开始时以源模型的权重初始化

##### 最近邻软投票
如图中 (a) 的特征空间所示，由于域偏移，当前分类器会对某些目标样本做出错误的决策。通过汇总附近点的知识，能够获得更可靠的估计，从而可能恢复正确的标签。使用弱增强图像 $t_w(x_t)$ 的特征向量 $w$ 并通过余弦相似度检索 $Q_w$ 中和其最接近的 N 个邻居

#### 联合自监督对比学习

##### 排除同类别的负对
目标图像的两种强增强版本被编码为 query 和 key 特征：$q = f_t(t_s(x_t))$，$k = f^{'}_t(t^{'}_s(x_t))$ 
使用一个长度为 P 的内存队列 $Q_s$ 存储特征 $\left \{ k^j \right \}_{j=1}^P$
InfoNCE 损失失致力于最小化 $q$ 和 $k$ 之间的余弦距离，同时最大化 $q$ 和 $Q_s$ 中每个 $k^j$ 之间的余弦距离
作者认为不应该推开相同类别的对，为此，$Q_s$ 中还存储了特征对应的伪标签，从而在负对中排除同类样本：
![image.png](https://papernote-1394983352.cos.ap-nanjing.myqcloud.com/tta-note-img/20260118181821874.png)

#### 额外正则化
##### 弱增强和强增强的一致性
利用弱增强图像的伪标签来“监督”模型对强增强版本的预测，也就是约束模型对强增强版本的预测和其弱增强版本的伪标签一致：
![image.png](https://papernote-1394983352.cos.ap-nanjing.myqcloud.com/tta-note-img/20260118181849331.png)

##### 多样性正则化
为了防止模型对错误伪标签的过度置信，在损失函数中使用正则化项来鼓励类别多样化：
![image.png](https://papernote-1394983352.cos.ap-nanjing.myqcloud.com/tta-note-img/20260118182210830.png)

整个框架最终的损失由交叉熵损失、对比损失和多样性正则化三项组成
